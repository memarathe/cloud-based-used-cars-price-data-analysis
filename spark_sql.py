# -*- coding: utf-8 -*-
"""spark_SQL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KFdJkyh4Kp6ocdh5biwzkEPAgUdE7nDF
"""

from pyspark.sql import SparkSession

# Step 1: Initialize Spark session
spark = SparkSession.builder.appName("UsedCarAnalysis").getOrCreate()

# Step 2: Read CSV file
file_path = "/content/part-00000-f6621328-5892-4709-89ba-3ff15e6b235f-c000.csv"  # Replace with the correct file path
df = spark.read.csv(file_path, header=True, inferSchema=True)

# Step 3: Create a temp SQL table
df.createOrReplaceTempView("car_data")

# Step 4: Define SQL queries (excluding rows where zip_code is NULL)
queries = {
    "Query 1: Top 5 Regions with the Highest Average Price":
    """
    SELECT zip_code, ROUND(AVG(price), 2) AS avg_price
    FROM car_data
    WHERE zip_code IS NOT NULL
    GROUP BY zip_code
    ORDER BY avg_price DESC
    LIMIT 5
    """,

    "Query 2: Total Car Sales Revenue by Year":
    """
    SELECT year, SUM(price) AS total_revenue
    FROM car_data
    WHERE zip_code IS NOT NULL
    GROUP BY year
    ORDER BY year ASC
    """,

    "Query 3: Count of Cars by Transmission Type per Region":
    """
    SELECT zip_code, transmission, COUNT(*) AS car_count
    FROM car_data
    WHERE zip_code IS NOT NULL
    GROUP BY zip_code, transmission
    ORDER BY zip_code
    """,

    "Query 4: Monthly Trend in Car Listings":
    """
    SELECT SUBSTRING(posting_date, 1, 7) AS month, COUNT(*) AS car_listings
    FROM car_data
    WHERE zip_code IS NOT NULL
    GROUP BY SUBSTRING(posting_date, 1, 7)
    ORDER BY month ASC
    """,

    "Query 5: Top 3 Car Types with the Highest Total Sales in Each Region":
    """
    SELECT zip_code, type, SUM(price) AS total_sales
    FROM car_data
    WHERE zip_code IS NOT NULL
    GROUP BY zip_code, type
    ORDER BY zip_code, total_sales DESC
    LIMIT 3
    """
}

# Step 5: Execute queries and print results
for query_name, query in queries.items():
    print(f"\n--- {query_name} ---")
    print(f"SQL Query Executed: {query}")
    result_df = spark.sql(query)
    result_df.show(truncate=False)  # Display full output without truncation

# Step 6: Stop Spark session
spark.stop()