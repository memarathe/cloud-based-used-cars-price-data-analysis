{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "k13ZjuWgYvGH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "680f8cd8-ced4-4ac2-a753-805fb03dc6a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "TVazPqj8q3F_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "W47NQ3HCrJ8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38adc2ec-4128-43ae-df77-0d8044226d15"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-14 00:50:59--  https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 388341449 (370M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.4.1-bin-hadoop3.tgz’\n",
            "\n",
            "spark-3.4.1-bin-had 100%[===================>] 370.35M  22.4MB/s    in 17s     \n",
            "\n",
            "2024-12-14 00:51:17 (21.9 MB/s) - ‘spark-3.4.1-bin-hadoop3.tgz’ saved [388341449/388341449]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf spark-3.4.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "4pfuhXFurc3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0090a345-16f2-4d02-858a-43540cf22703"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spark-3.4.1-bin-hadoop3/\n",
            "spark-3.4.1-bin-hadoop3/R/\n",
            "spark-3.4.1-bin-hadoop3/R/lib/\n",
            "spark-3.4.1-bin-hadoop3/R/lib/sparkr.zip\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/html/\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/html/R.css\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/html/00Index.html\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/INDEX\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/help/\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/help/aliases.rds\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/help/AnIndex\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/help/paths.rds\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/worker/\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/worker/worker.R\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/worker/daemon.R\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/tests/\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/tests/testthat/\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/profile/\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/profile/shell.R\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/profile/general.R\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/doc/\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/doc/index.html\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/doc/sparkr-vignettes.html\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/doc/sparkr-vignettes.Rmd\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/doc/sparkr-vignettes.R\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/R/\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/R/SparkR\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/Meta/\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/Meta/vignette.rds\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/Meta/links.rds\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/Meta/features.rds\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/Meta/package.rds\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/DESCRIPTION\n",
            "spark-3.4.1-bin-hadoop3/R/lib/SparkR/NAMESPACE\n",
            "spark-3.4.1-bin-hadoop3/sbin/\n",
            "spark-3.4.1-bin-hadoop3/sbin/workers.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/stop-workers.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/stop-worker.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/stop-thriftserver.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/stop-slaves.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/stop-slave.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/stop-mesos-dispatcher.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/stop-master.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/stop-history-server.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/stop-connect-server.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/stop-all.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/start-workers.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/start-worker.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/start-thriftserver.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/start-slaves.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/start-slave.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/start-mesos-shuffle-service.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/start-mesos-dispatcher.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/start-master.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/start-history-server.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/start-connect-server.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/start-all.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/spark-daemons.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/spark-daemon.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/spark-config.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/slaves.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/decommission-worker.sh\n",
            "spark-3.4.1-bin-hadoop3/sbin/decommission-slave.sh\n",
            "spark-3.4.1-bin-hadoop3/python/\n",
            "spark-3.4.1-bin-hadoop3/python/dist/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark.egg-info/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark.egg-info/SOURCES.txt\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark.egg-info/top_level.txt\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark.egg-info/requires.txt\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark.egg-info/dependency_links.txt\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark.egg-info/PKG-INFO\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/userlibrary.py\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/userlib-0.1.zip\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/test_pytorch_training_file.py\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/text-test.txt\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/streaming/\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/streaming/text-test.txt\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/people_array_utf16le.json\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/people_array.json\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/people1.json\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/people.json\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/orc_partitioned/\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/sql/ages_newlines.csv\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/hello/\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/hello/sub_hello/\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/hello/hello.txt\n",
            "spark-3.4.1-bin-hadoop3/python/test_support/SimpleHTTPServer.py\n",
            "spark-3.4.1-bin-hadoop3/python/test_coverage/\n",
            "spark-3.4.1-bin-hadoop3/python/test_coverage/sitecustomize.py\n",
            "spark-3.4.1-bin-hadoop3/python/test_coverage/coverage_daemon.py\n",
            "spark-3.4.1-bin-hadoop3/python/test_coverage/conf/\n",
            "spark-3.4.1-bin-hadoop3/python/test_coverage/conf/spark-defaults.conf\n",
            "spark-3.4.1-bin-hadoop3/python/setup.cfg\n",
            "spark-3.4.1-bin-hadoop3/python/run-tests.py\n",
            "spark-3.4.1-bin-hadoop3/python/run-tests-with-coverage\n",
            "spark-3.4.1-bin-hadoop3/python/docs/\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/user_guide/\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/user_guide/python_packaging.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/types.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/transform_apply.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/pandas_pyspark.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/options.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/index.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/from_to_dbms.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/faq.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/typehints.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/best_practices.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/user_guide/index.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/user_guide/arrow_pandas.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/user_guide/sql/\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/user_guide/sql/index.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/user_guide/sql/arrow_pandas.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.streaming.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.ss/\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.ss/query_management.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.ss/io.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.ss/index.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.ss/core_classes.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/window.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/udf.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/row.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/protobuf.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/observation.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/io.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/grouping.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/configuration.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/column.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/catalog.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/avro.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/spark_session.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/index.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/functions.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/dataframe.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/data_types.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.sql/core_classes.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.resource.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/window.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/series.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/resampling.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/ml.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/io.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/indexing.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/index.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/groupby.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/general_functions.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/extensions.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.pandas/frame.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.mllib.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.ml.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/pyspark.errors.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/reference/index.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/migration_guide/\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/migration_guide/koalas_to_pyspark.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/migration_guide/index.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/migration_guide/pyspark_upgrade.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/index.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/getting_started/\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/getting_started/quickstart_ps.ipynb\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/getting_started/quickstart_df.ipynb\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/getting_started/quickstart_connect.ipynb\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/getting_started/index.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/getting_started/install.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/development/\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/development/testing.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/development/setting_ide.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/development/debugging.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/development/index.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/development/contributing.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/conf.py\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/_templates/\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/_templates/autosummary/\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/_templates/autosummary/class_with_docs.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/_templates/autosummary/class.rst\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/_static/\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/_static/css/\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/_static/css/pyspark.css\n",
            "spark-3.4.1-bin-hadoop3/python/docs/source/_static/copybutton.js\n",
            "spark-3.4.1-bin-hadoop3/python/docs/make2.bat\n",
            "spark-3.4.1-bin-hadoop3/python/docs/make.bat\n",
            "spark-3.4.1-bin-hadoop3/python/docs/Makefile\n",
            "spark-3.4.1-bin-hadoop3/python/README.md\n",
            "spark-3.4.1-bin-hadoop3/python/MANIFEST.in\n",
            "spark-3.4.1-bin-hadoop3/python/.gitignore\n",
            "spark-3.4.1-bin-hadoop3/python/.coveragerc\n",
            "spark-3.4.1-bin-hadoop3/python/mypy.ini\n",
            "spark-3.4.1-bin-hadoop3/python/setup.py\n",
            "spark-3.4.1-bin-hadoop3/python/run-tests\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/torch/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/torch/distributor.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/torch/torch_run_process_wrapper.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/torch/tests/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/torch/tests/test_log_communication.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/torch/tests/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/torch/tests/test_distributor.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/torch/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/torch/log_communication.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/typing/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/typing/test_regression.yml\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/typing/test_readable.yml\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/typing/test_param.yml\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/typing/test_feature.yml\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/typing/test_evaluation.yml\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/typing/test_clustering.yaml\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/typing/test_classification.yml\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/test_wrapper.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/test_util.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/test_training_summary.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/test_stat.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/test_pipeline.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/test_persistence.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/test_param.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/test_model_cache.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/test_linalg.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/test_image.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/test_feature.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/test_evaluation.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/test_base.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/test_algorithms.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/test_tuning.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tests/test_functions.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/regression.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/recommendation.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/util.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/stat.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/functions.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/fpm.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/feature.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/pipeline.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/param/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/param/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/param/shared.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/model_cache.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/linalg/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/linalg/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/image.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/evaluation.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/common.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/clustering.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/classification.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/base.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/_typing.pyi\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/wrapper.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tuning.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/ml/tree.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/join.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/instrumentation_utils.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/find_spark_home.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/files.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/errors/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/errors/tests/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/errors/tests/test_errors.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/errors/tests/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/errors/exceptions/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/errors/exceptions/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/errors/exceptions/connect.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/errors/exceptions/base.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/errors/utils.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/errors/error_classes.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/errors/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/daemon.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/cloudpickle/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/cloudpickle/compat.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/cloudpickle/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/_typing.pyi\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/_globals.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/python/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/python/pyspark/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/python/pyspark/shell.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/__pycache__/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/__pycache__/install.cpython-38.pyc\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/traceback_utils.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/typing/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/typing/test_resultiterable.yml\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/typing/test_rdd.yml\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/typing/test_core.yml\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/typing/test_context.yml\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_worker.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_util.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_taskcontext.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_statcounter.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_stage_sched.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_shuffle.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_serializers.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_readwrite.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_rddsampler.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_rddbarrier.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_rdd.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_pin_thread.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_memory_profiler.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_join.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_daemon.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_conf.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_broadcast.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_appsubmit.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_profiler.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_install_spark.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/tests/test_context.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/testing/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/testing/utils.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/testing/streamingutils.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/testing/sqlutils.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/testing/pandasutils.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/testing/mlutils.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/testing/mllibutils.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/testing/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/testing/connectutils.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/streaming/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/streaming/util.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/streaming/tests/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/streaming/tests/test_listener.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/streaming/tests/test_kinesis.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/streaming/tests/test_dstream.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/streaming/tests/test_context.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/streaming/tests/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/streaming/listener.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/streaming/kinesis.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/streaming/dstream.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/streaming/context.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/streaming/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/storagelevel.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/status.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/statcounter.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/plot/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/plot/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/plot/test_series_plot_plotly.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/plot/test_series_plot_matplotlib.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/plot/test_series_plot.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/plot/test_frame_plot_plotly.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/plot/test_frame_plot_matplotlib.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/plot/test_frame_plot.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/indexes/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/indexes/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_timedelta.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_datetime.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_category.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_base.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/testing_utils.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_udt_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_timedelta_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_string_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_num_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_null_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_datetime_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_date_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_complex_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_categorical_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_boolean_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_binary_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_base.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_window.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_utils.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_typedef.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_stats.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_sql.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_spark_functions.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_series_string.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_series_datetime.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_series_conversion.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_series.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_scalars.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_rolling.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_reshape.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_resample.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_repr.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames_slow.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby_rolling.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby_expanding.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_numpy_compat.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_namespace.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_internal.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_indexops_spark.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_indexing.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_groupby_slow.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_groupby.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_generic_functions.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_frame_spark.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_extension.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_expanding.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_ewm.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_default_index.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_dataframe_spark_io.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_dataframe_slow.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_dataframe_conversion.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_dataframe.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_csv.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_config.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/tests/test_categorical.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/spark/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/spark/utils.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/spark/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/spark/functions.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/spark/accessors.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/resample.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/plot/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/plot/plotly.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/plot/core.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/plot/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/plot/matplotlib.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/mlflow.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/missing/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/missing/window.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/missing/series.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/missing/scalars.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/missing/resample.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/missing/indexes.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/missing/groupby.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/missing/general_functions.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/missing/frame.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/missing/common.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/missing/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/indexes/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/indexes/timedelta.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/indexes/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/indexes/numeric.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/indexes/multi.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/indexes/datetimes.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/indexes/category.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/indexes/base.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/extensions.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/exceptions.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/datetimes.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/udt_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/complex_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/timedelta_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/string_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/num_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/null_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/datetime_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/date_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/categorical_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/boolean_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/binary_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/data_type_ops/base.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/correlation.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/categorical.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/accessors.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/window.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/usage_logging/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/usage_logging/usage_logger.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/usage_logging/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/typedef/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/typedef/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/typedef/typehints.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/utils.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/supported_api_gen.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/strings.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/sql_processor.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/sql_formatter.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/series.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/numpy_compat.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/namespace.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/internal.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/indexing.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/groupby.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/generic.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/frame.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/config.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/base.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/pandas/_typing.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/util.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/tree.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/tests/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/tests/test_util.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/tests/test_streaming_algorithms.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/tests/test_stat.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/tests/test_linalg.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/tests/test_feature.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/tests/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/tests/test_algorithms.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/stat/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/stat/test.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/stat/distribution.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/stat/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/regression.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/recommendation.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/random.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/linalg/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/fpm.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/feature.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/evaluation.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/common.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/classification.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/_typing.pyi\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/mllib/clustering.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/window.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/typing/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/typing/test_udf.yml\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/typing/test_session.yml\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/typing/test_readwriter.yml\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/typing/test_functions.yml\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/typing/test_dataframe.yml\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/typing/test_column.yml\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_utils.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_udf_profiler.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_session.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_serde.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_readwriter.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_pandas_sqlmetrics.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_group.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_errors.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_datasources.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_context.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_conf.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/streaming/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/streaming/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/streaming/test_streaming_listener.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/streaming/test_streaming.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/pandas/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_udf_typehints_with_future_annotations.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/pandas/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_udf_window.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_udf_typehints.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_udf_scalar.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_udf_grouped_agg.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_udf.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_map.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_grouped_map_with_state.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_grouped_map.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_cogrouped_map.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_udf.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_serde.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_readwriter.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_pandas_map.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_group.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_functions.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_errors.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_datasources.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_conf.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_column.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_catalog.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_arrow_map.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_types.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_pandas_udf.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_pandas_grouped_map.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_pandas_cogrouped_map.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_dataframe.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_arrow.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_connect_plan.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_connect_function.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_connect_column.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_connect_basic.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/connect/test_client.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_udf.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_types.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_functions.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_dataframe.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_column.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_catalog.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_arrow_python_udf.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_arrow_map.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/tests/test_arrow.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/streaming/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/streaming/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/streaming/state.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/streaming/readwriter.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/streaming/query.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/streaming/listener.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/sql_formatter.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/protobuf/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/protobuf/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/protobuf/functions.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/pandas/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/pandas/utils.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/pandas/functions.pyi\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/pandas/_typing/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/series.pyi\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/frame.pyi\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/__init__.pyi\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/pandas/_typing/__init__.pyi\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/pandas/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/pandas/types.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/pandas/typehints.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/pandas/serializers.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/pandas/map_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/pandas/group_ops.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/pandas/functions.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/pandas/conversion.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/observation.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/group.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/utils.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/types_pb2.pyi\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/example_plugins_pb2.pyi\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/types_pb2.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/relations_pb2.pyi\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/relations_pb2.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/expressions_pb2.pyi\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/expressions_pb2.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/example_plugins_pb2.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/common_pb2.pyi\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/common_pb2.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/commands_pb2.pyi\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/commands_pb2.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/catalog_pb2.pyi\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/catalog_pb2.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/base_pb2_grpc.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/base_pb2.pyi\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/proto/base_pb2.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/conf.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/window.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/udf.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/types.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/session.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/readwriter.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/plan.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/group.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/functions.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/expressions.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/dataframe.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/conversion.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/column.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/client.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/catalog.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/connect/_typing.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/conf.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/avro/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/avro/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/avro/functions.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/utils.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/udf.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/types.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/session.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/readwriter.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/functions.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/dataframe.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/context.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/column.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/catalog.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/_typing.pyi\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/sql/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/shuffle.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/resultiterable.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/resource/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/resource/tests/\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/resource/tests/test_resources.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/resource/tests/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/resource/requests.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/resource/profile.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/resource/information.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/resource/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/rddsampler.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/py.typed\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/rdd.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/profiler.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/java_gateway.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/install.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/context.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/conf.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/broadcast.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/accumulators.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/worker.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/version.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/util.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/taskcontext.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/shell.py\n",
            "spark-3.4.1-bin-hadoop3/python/pyspark/serializers.py\n",
            "spark-3.4.1-bin-hadoop3/python/lib/\n",
            "spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip\n",
            "spark-3.4.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip\n",
            "spark-3.4.1-bin-hadoop3/python/lib/PY4J_LICENSE.txt\n",
            "spark-3.4.1-bin-hadoop3/bin/\n",
            "spark-3.4.1-bin-hadoop3/bin/sparkR2.cmd\n",
            "spark-3.4.1-bin-hadoop3/bin/sparkR.cmd\n",
            "spark-3.4.1-bin-hadoop3/bin/sparkR\n",
            "spark-3.4.1-bin-hadoop3/bin/spark-submit2.cmd\n",
            "spark-3.4.1-bin-hadoop3/bin/spark-submit.cmd\n",
            "spark-3.4.1-bin-hadoop3/bin/spark-submit\n",
            "spark-3.4.1-bin-hadoop3/bin/spark-sql2.cmd\n",
            "spark-3.4.1-bin-hadoop3/bin/spark-sql.cmd\n",
            "spark-3.4.1-bin-hadoop3/bin/spark-sql\n",
            "spark-3.4.1-bin-hadoop3/bin/spark-shell2.cmd\n",
            "spark-3.4.1-bin-hadoop3/bin/spark-shell.cmd\n",
            "spark-3.4.1-bin-hadoop3/bin/spark-shell\n",
            "spark-3.4.1-bin-hadoop3/bin/spark-connect-shell\n",
            "spark-3.4.1-bin-hadoop3/bin/spark-class2.cmd\n",
            "spark-3.4.1-bin-hadoop3/bin/spark-class.cmd\n",
            "spark-3.4.1-bin-hadoop3/bin/spark-class\n",
            "spark-3.4.1-bin-hadoop3/bin/run-example.cmd\n",
            "spark-3.4.1-bin-hadoop3/bin/run-example\n",
            "spark-3.4.1-bin-hadoop3/bin/pyspark2.cmd\n",
            "spark-3.4.1-bin-hadoop3/bin/pyspark.cmd\n",
            "spark-3.4.1-bin-hadoop3/bin/pyspark\n",
            "spark-3.4.1-bin-hadoop3/bin/load-spark-env.sh\n",
            "spark-3.4.1-bin-hadoop3/bin/load-spark-env.cmd\n",
            "spark-3.4.1-bin-hadoop3/bin/find-spark-home.cmd\n",
            "spark-3.4.1-bin-hadoop3/bin/find-spark-home\n",
            "spark-3.4.1-bin-hadoop3/bin/docker-image-tool.sh\n",
            "spark-3.4.1-bin-hadoop3/bin/beeline.cmd\n",
            "spark-3.4.1-bin-hadoop3/bin/beeline\n",
            "spark-3.4.1-bin-hadoop3/README.md\n",
            "spark-3.4.1-bin-hadoop3/conf/\n",
            "spark-3.4.1-bin-hadoop3/conf/workers.template\n",
            "spark-3.4.1-bin-hadoop3/conf/spark-env.sh.template\n",
            "spark-3.4.1-bin-hadoop3/conf/spark-defaults.conf.template\n",
            "spark-3.4.1-bin-hadoop3/conf/metrics.properties.template\n",
            "spark-3.4.1-bin-hadoop3/conf/log4j2.properties.template\n",
            "spark-3.4.1-bin-hadoop3/conf/fairscheduler.xml.template\n",
            "spark-3.4.1-bin-hadoop3/data/\n",
            "spark-3.4.1-bin-hadoop3/data/streaming/\n",
            "spark-3.4.1-bin-hadoop3/data/streaming/AFINN-111.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/sample_svm_data.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/sample_movielens_data.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/sample_linear_regression_data.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/sample_libsvm_data.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/sample_lda_data.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/sample_kmeans_data.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/sample_fpgrowth.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/sample_binary_classification_data.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/ridge-data/\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/ridge-data/lpsa.data\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/pic_data.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/pagerank_data.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/kmeans_data.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/images/\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/images/origin/\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/images/origin/multi-channel/\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/images/origin/multi-channel/grayscale.jpg\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/images/origin/multi-channel/BGRA.png\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/images/origin/license.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/images/origin/kittens/\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/images/origin/kittens/not-image.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/images/origin/kittens/DP802813.jpg\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/images/origin/kittens/DP153539.jpg\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/images/origin/kittens/54893.jpg\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/images/license.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/gmm_data.txt\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/als/\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/als/test.data\n",
            "spark-3.4.1-bin-hadoop3/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-3.4.1-bin-hadoop3/data/graphx/\n",
            "spark-3.4.1-bin-hadoop3/data/graphx/users.txt\n",
            "spark-3.4.1-bin-hadoop3/data/graphx/followers.txt\n",
            "spark-3.4.1-bin-hadoop3/NOTICE\n",
            "spark-3.4.1-bin-hadoop3/licenses/\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-zstd.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-zstd-jni.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-xmlenc.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-vis-timeline.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-spire.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-sorttable.js.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-slf4j.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-scopt.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-respond.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-reflectasm.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-re2j.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-pyrolite.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-py4j.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-protobuf.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-pmml-model.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-paranamer.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-mustache.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-modernizr.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-minlog.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-matchMedia-polyfill.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-machinist.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-leveldbjni.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-kryo.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-jsp-api.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-json-formatter.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-jquery.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-join.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-jodd.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-jline.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-jaxb-runtime.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-javolution.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-javax-transaction-transaction-api.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-javassist.html\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-janino.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-jakarta.xml.bind-api.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-jakarta.activation-api.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-jakarta-ws-rs-api\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-jakarta-annotation-api\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-istack-commons-runtime.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-f2j.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-dnsjava.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-datatables.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-dagre-d3.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-d3.min.js.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-cloudpickle.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-bootstrap.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-blas.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-automaton.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-arpack.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-antlr.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-JTransforms.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-JLargeArrays.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-CC0.txt\n",
            "spark-3.4.1-bin-hadoop3/licenses/LICENSE-AnchorJS.txt\n",
            "spark-3.4.1-bin-hadoop3/LICENSE\n",
            "spark-3.4.1-bin-hadoop3/examples/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/AccumulatorMetricsTest.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKerberizedKafkaWordCount.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKerberizedKafkaWordCount.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredComplexSessionization.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/jdbc/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/jdbc/ExampleJdbcConnectionProvider.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedScalar.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VarianceThresholdSelectorExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/UnivariateFeatureSelectorExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RobustScalerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FMRegressorExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FMClassifierExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/SparkSessionExtensionsTest.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/SessionExtensionsWithoutLoader.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/SessionExtensionsWithLoader.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/AgeExample.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/MiniReadWriteTest.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/users.parquet\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/users.orc\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/users.avro\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/user.avsc\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/people.txt\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/people.json\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/people.csv\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/kv1.txt\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/full_user.avsc\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/employees.json\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/dir1/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/dir1/file3.json\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/dir1/file1.parquet\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/dir1/dir2/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/dir1/dir2/file2.parquet\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/META-INF/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/META-INF/services/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/META-INF/services/org.apache.spark.sql.jdbc.JdbcConnectionProvider\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/resources/META-INF/services/org.apache.spark.sql.SparkSessionExtensionsProvider\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/streaming/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/svmLinear.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/survreg.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/randomForest.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/prefixSpan.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/powerIterationClustering.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/mlp.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/ml.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/logit.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/lm_with_elastic_net.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/lda.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/kstest.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/kmeans.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/isoreg.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/glm.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/gbt.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/fpm.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/fmRegressor.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/fmClassifier.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/decisionTree.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/ml/als.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/dataframe.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/data-manipulation.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/wordcount.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/transitive_closure.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/streaming/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/streaming/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/status_api_demo.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/sql/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/sql/streaming/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/sql/streaming/structured_sessionization.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/sql/streaming/structured_network_wordcount_session_window.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/sql/streaming/__init__,py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/sql/hive.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/sql/datasource.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/sql/basic.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/sql/arrow.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/sql/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/sort.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/pi.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/parquet_inputformat.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/pagerank.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/word2vec.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/svd_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/kmeans.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/correlations.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/mllib/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/vector_size_hint_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/variance_threshold_selector_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/univariate_feature_selector_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/summarizer_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/robust_scaler_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/rformula_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/prefixspan_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/power_iteration_clustering_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/pca_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/onehot_encoder_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/linearsvc.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/lda_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/interaction_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/imputer_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/fm_regressor_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/fm_classifier_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/feature_hasher_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/dct_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/cross_validator.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/correlation_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/als_example.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/ml/__init__,py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/logistic_regression.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/kmeans.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/avro_inputformat.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/als.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/python/__init__.py\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKerberizedKafkaWordCount.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKerberizedKafkaWordCount.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredComplexSessionization.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedScalar.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVarianceThresholdSelectorExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaUnivariateFeatureSelectorExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFMRegressorExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scripts/\n",
            "spark-3.4.1-bin-hadoop3/examples/src/main/scripts/getGpusResources.sh\n",
            "spark-3.4.1-bin-hadoop3/examples/jars/\n",
            "spark-3.4.1-bin-hadoop3/examples/jars/spark-examples_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/examples/jars/scopt_2.12-3.7.1.jar\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/tests/\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/tests/worker_memory_check.py\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/tests/python_executable_check.py\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/tests/pyfiles.py\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/tests/py_container_checks.py\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/tests/decommissioning_cleanup.py\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/tests/decommissioning.py\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/tests/autoscale.py\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/dockerfiles/\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/dockerfiles/spark/\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/dockerfiles/spark/entrypoint.sh\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/dockerfiles/spark/decom.sh\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/python/\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/R/\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n",
            "spark-3.4.1-bin-hadoop3/kubernetes/dockerfiles/spark/Dockerfile\n",
            "spark-3.4.1-bin-hadoop3/yarn/\n",
            "spark-3.4.1-bin-hadoop3/yarn/spark-3.4.1-yarn-shuffle.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/\n",
            "spark-3.4.1-bin-hadoop3/jars/rocksdbjni-7.9.2.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/py4j-0.10.9.7.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/protobuf-java-2.5.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/pickle-1.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/parquet-jackson-1.12.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/parquet-hadoop-1.12.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/parquet-format-structures-1.12.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/parquet-encoding-1.12.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/parquet-common-1.12.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/parquet-column-1.12.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/paranamer-2.8.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/osgi-resource-locator-1.0.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/oro-2.0.8.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/orc-shims-1.8.4.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/orc-mapreduce-1.8.4-shaded-protobuf.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/orc-core-1.8.4-shaded-protobuf.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/opencsv-2.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/okio-1.15.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/okhttp-3.12.12.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/objenesis-3.2.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/netty-transport-native-unix-common-4.1.87.Final.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/netty-transport-native-kqueue-4.1.87.Final-osx-x86_64.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/netty-transport-native-kqueue-4.1.87.Final-osx-aarch_64.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/netty-transport-native-epoll-4.1.87.Final-linux-x86_64.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/netty-transport-native-epoll-4.1.87.Final-linux-aarch_64.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/netty-transport-classes-kqueue-4.1.87.Final.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/netty-transport-classes-epoll-4.1.87.Final.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/netty-transport-4.1.87.Final.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/netty-resolver-4.1.87.Final.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/netty-handler-proxy-4.1.87.Final.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/netty-handler-4.1.87.Final.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/netty-common-4.1.87.Final.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/netty-codec-socks-4.1.87.Final.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/netty-codec-http2-4.1.87.Final.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/netty-codec-http-4.1.87.Final.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/netty-codec-4.1.87.Final.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/netty-buffer-4.1.87.Final.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/netty-all-4.1.87.Final.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/minlog-1.3.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/metrics-jvm-4.2.15.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/metrics-json-4.2.15.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/metrics-jmx-4.2.15.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/metrics-graphite-4.2.15.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/metrics-core-4.2.15.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/mesos-1.4.3-shaded-protobuf.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/lz4-java-1.8.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/logging-interceptor-3.12.12.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/log4j-slf4j2-impl-2.19.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/log4j-core-2.19.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/log4j-api-2.19.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/log4j-1.2-api-2.19.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/libthrift-0.12.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/libfb303-0.9.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/leveldbjni-all-1.8.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/lapack-3.0.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-storageclass-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-scheduling-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-rbac-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-policy-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-node-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-networking-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-metrics-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-gatewayapi-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-flowcontrol-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-extensions-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-events-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-discovery-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-core-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-coordination-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-common-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-certificates-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-batch-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-autoscaling-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-apps-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-apiextensions-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-model-admissionregistration-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-httpclient-okhttp-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-client-api-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kubernetes-client-6.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/kryo-shaded-4.0.2.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jul-to-slf4j-2.0.6.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jta-1.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jsr305-3.0.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/json4s-scalap_2.12-3.7.0-M11.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/json4s-jackson_2.12-3.7.0-M11.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/json4s-core_2.12-3.7.0-M11.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/json4s-ast_2.12-3.7.0-M11.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/json-1.8.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jpam-1.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jodd-core-3.5.2.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/joda-time-2.12.2.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jline-2.14.6.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jersey-server-2.36.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jersey-hk2-2.36.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jersey-container-servlet-core-2.36.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jersey-container-servlet-2.36.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jersey-common-2.36.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jersey-client-2.36.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jdo-api-3.0.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jcl-over-slf4j-2.0.6.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jaxb-runtime-2.3.2.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/javolution-5.5.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/javax.jdo-3.2.0-m3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/javassist-3.25.0-GA.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/janino-3.1.9.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jakarta.xml.bind-api-2.3.2.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jakarta.ws.rs-api-2.1.6.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jakarta.validation-api-2.0.2.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jakarta.servlet-api-4.0.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jakarta.inject-2.6.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jakarta.annotation-api-1.3.5.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jackson-module-scala_2.12-2.14.2.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jackson-datatype-jsr310-2.14.2.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jackson-dataformat-yaml-2.14.2.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jackson-databind-2.14.2.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jackson-core-2.14.2.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/jackson-annotations-2.14.2.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/ivy-2.5.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/istack-commons-runtime-3.0.8.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/httpcore-4.4.16.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/httpclient-4.5.14.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hk2-utils-2.6.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hk2-locator-2.6.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hk2-api-2.6.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hive-storage-api-2.8.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hive-shims-scheduler-2.3.9.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hive-shims-common-2.3.9.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hive-shims-2.3.9.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hive-shims-0.23-2.3.9.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hive-service-rpc-3.1.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hive-serde-2.3.9.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hive-metastore-2.3.9.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hive-llap-common-2.3.9.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hive-jdbc-2.3.9.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hive-exec-2.3.9-core.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hive-common-2.3.9.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hive-cli-2.3.9.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hive-beeline-2.3.9.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hadoop-yarn-server-web-proxy-3.3.4.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hadoop-shaded-guava-1.1.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hadoop-client-runtime-3.3.4.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/hadoop-client-api-3.3.4.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/guava-14.0.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/gson-2.2.4.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/flatbuffers-java-1.12.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/derby-10.14.2.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/datanucleus-rdbms-4.1.19.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/datanucleus-core-4.1.17.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/datanucleus-api-jdo-4.2.4.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/curator-recipes-2.13.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/curator-framework-2.13.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/curator-client-2.13.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/compress-lzf-1.1.2.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/commons-text-1.10.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/commons-pool-1.5.4.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/commons-math3-3.6.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/commons-logging-1.1.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/commons-lang3-3.12.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/commons-lang-2.6.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/commons-io-2.11.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/commons-dbcp-1.4.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/commons-crypto-1.1.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/commons-compress-1.22.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/commons-compiler-3.1.9.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/commons-collections4-4.4.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/commons-collections-3.2.2.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/commons-codec-1.15.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/commons-cli-1.5.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/chill_2.12-0.10.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/chill-java-0.10.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/cats-kernel_2.12-2.1.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/breeze_2.12-2.1.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/breeze-macros_2.12-2.1.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/blas-3.0.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/avro-mapred-1.11.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/avro-ipc-1.11.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/avro-1.11.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/audience-annotations-0.5.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/arrow-vector-11.0.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/arrow-memory-netty-11.0.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/arrow-memory-core-11.0.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/arrow-format-11.0.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/arpack_combined_all-0.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/arpack-3.0.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/aopalliance-repackaged-2.6.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/antlr4-runtime-4.9.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/antlr-runtime-3.5.2.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/annotations-17.0.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/algebra_2.12-2.0.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/aircompressor-0.21.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/activation-1.1.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/ST4-4.0.4.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/RoaringBitmap-0.9.38.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/JTransforms-3.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/JLargeArrays-1.5.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/HikariCP-2.5.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/zstd-jni-1.5.2-5.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/zookeeper-jute-3.6.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/zookeeper-3.6.3.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/zjsonpatch-0.3.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/xz-1.9.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/xbean-asm9-shaded-4.22.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/univocity-parsers-2.9.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/transaction-api-1.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/tink-1.7.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/threeten-extra-1.7.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/super-csv-2.2.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/stream-2.9.6.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/stax-api-1.0.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spire_2.12-0.17.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spire-util_2.12-0.17.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spire-platform_2.12-0.17.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spire-macros_2.12-0.17.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-yarn_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-unsafe_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-tags_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-streaming_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-sql_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-sketch_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-repl_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-network-shuffle_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-network-common_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-mllib_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-mllib-local_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-mesos_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-launcher_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-kvstore_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-kubernetes_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-hive_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-hive-thriftserver_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-graphx_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-core_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/spark-catalyst_2.12-3.4.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/snappy-java-1.1.10.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/snakeyaml-1.33.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/slf4j-api-2.0.6.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/shims-0.9.38.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/scala-xml_2.12-2.1.0.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/scala-reflect-2.12.17.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/scala-parser-combinators_2.12-2.1.1.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/scala-library-2.12.17.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/scala-compiler-2.12.17.jar\n",
            "spark-3.4.1-bin-hadoop3/jars/scala-collection-compat_2.12-2.7.0.jar\n",
            "spark-3.4.1-bin-hadoop3/RELEASE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "N0LiLT7irg5Y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set environment variables for Java and Spark\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"  # Update if path changes\n"
      ],
      "metadata": {
        "id": "3A0XxzvvrskP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Qi1yY2_5XflP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec69e13b-e910-4a2c-8351-b4aecdeec3ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting awscli\n",
            "  Downloading awscli-1.36.22-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting botocore==1.35.81 (from awscli)\n",
            "  Downloading botocore-1.35.81-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting docutils<0.17,>=0.10 (from awscli)\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from awscli)\n",
            "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: PyYAML<6.1,>=3.10 in /usr/local/lib/python3.10/dist-packages (from awscli) (6.0.2)\n",
            "Collecting colorama<0.4.7,>=0.2.5 (from awscli)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting rsa<4.8,>=3.1.2 (from awscli)\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from botocore==1.35.81->awscli)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore==1.35.81->awscli) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore==1.35.81->awscli) (2.2.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.35.81->awscli) (1.17.0)\n",
            "Downloading awscli-1.36.22-py3-none-any.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.35.81-py3-none-any.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.2/548.2 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Downloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: rsa, jmespath, docutils, colorama, botocore, s3transfer, awscli\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "Successfully installed awscli-1.36.22 botocore-1.35.81 colorama-0.4.6 docutils-0.16 jmespath-1.0.1 rsa-4.7.2 s3transfer-0.10.4\n",
            "Collecting geopy\n",
            "  Downloading geopy-2.4.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting geographiclib<3,>=1.52 (from geopy)\n",
            "  Downloading geographiclib-2.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading geopy-2.4.1-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading geographiclib-2.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: geographiclib, geopy\n",
            "Successfully installed geographiclib-2.0 geopy-2.4.1\n",
            "Collecting tabulate\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: tabulate\n",
            "Successfully installed tabulate-0.9.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.35.81-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: botocore<1.36.0,>=1.35.81 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.35.81)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3) (0.10.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.81->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.81->boto3) (2.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.81->boto3) (1.17.0)\n",
            "Downloading boto3-1.35.81-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: boto3\n",
            "Successfully installed boto3-1.35.81\n"
          ]
        }
      ],
      "source": [
        "!pip install awscli\n",
        "# !pip install pyspark\n",
        "# # Install Geopy for geolocation services (used to extract zip codes)\n",
        "!pip install geopy\n",
        "# # Install Tabulate for pretty table formatting\n",
        "!pip install tabulate\n",
        "!pip install numpy\n",
        "!pip install scikit-learn\n",
        "!pip install pandas\n",
        "!pip install boto3\n",
        "# !pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Start a Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local\") \\\n",
        "    .appName(\"GoogleColabSpark\") \\\n",
        "    .config('spark.ui.port', '4050') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Verify Spark version\n",
        "print(spark.version)"
      ],
      "metadata": {
        "id": "85XlHoizrvlX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96320b58-15d4-4f36-e0c0-eb98ec7d792b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import logging\n",
        "import os\n",
        "from tabulate import tabulate  # Import tabulate for better table formatting\n",
        "\n",
        "# Set up the logger\n",
        "log_file = 'output.log'\n",
        "\n",
        "# If the log file already exists, remove it to create a fresh file\n",
        "if os.path.exists(log_file):\n",
        "    os.remove(log_file)\n",
        "\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Create a file handler to write log messages to a file (overwrite the log file each time)\n",
        "# Specify utf-8 encoding to handle special characters\n",
        "file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')  # Use 'w' mode to overwrite the log file\n",
        "file_handler.setLevel(logging.INFO)\n",
        "\n",
        "# Create a log formatter\n",
        "formatter = logging.Formatter('%(message)s')\n",
        "file_handler.setFormatter(formatter)\n",
        "\n",
        "# Add the file handler to the logger\n",
        "logger.addHandler(file_handler)\n",
        "\n",
        "# Function to log and print information messages (not DataFrame content)\n",
        "def log_message(message=None, title=None):\n",
        "    if title:\n",
        "        logger.info(f\"=== {title} ===\")\n",
        "    if message:\n",
        "        logger.info(message)\n",
        "    #print(message)  # Optionally print to the console for local execution\n",
        "\n",
        "# Function to log and print the output of show(5) from DataFrame in a table format\n",
        "def log_show_output(spark_df=None, title=None):\n",
        "    if spark_df and title:\n",
        "        logger.info(f\"=== {title} ===\")\n",
        "        # Get the column names from the DataFrame\n",
        "        columns = spark_df.columns\n",
        "        # Get the top 5 rows\n",
        "        rows = spark_df.take(5)\n",
        "        # Format the rows into a list of lists, ready for tabulation\n",
        "        formatted_rows = [list(row) for row in rows]\n",
        "        # Log the table as a string using tabulate\n",
        "        table = tabulate(formatted_rows, headers=columns, tablefmt=\"grid\")\n",
        "        # Log and print the table\n",
        "        logger.info(table)\n",
        "        #print(table)  # Optionally print to the console for local execution"
      ],
      "metadata": {
        "id": "VQO5w0jjYP6s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import boto3\n",
        "import pandas as pd\n",
        "import io  # Import the io module to use StringIO\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# # Set up AWS credentials as environment variables (this can be done securely)\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = 'AKIAVPEYWO2O7N5SVEMK'\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = '7goDXe1pF7ogmk4sj3VaU71f1woWx3BTrKPGOqnt'\n",
        "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'  # Replace with your AWS region\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BDA Mini Project\") \\\n",
        "    .config(\"spark.executor.memory\", \"16g\") \\\n",
        "    .config(\"spark.driver.memory\", \"16g\") \\\n",
        "    .config(\"spark.kryoserializer.buffer\", \"512m\") \\\n",
        "    .config(\"spark.kryoserializer.buffer.max\", \"1024m\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
        "    .config(\"spark.default.parallelism\", \"100\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Use Boto3 to interact with S3 (initialize a session)\n",
        "s3_client = boto3.client('s3')\n",
        "\n",
        "# Define the S3 bucket and file\n",
        "bucket_name = 'rmarathe-raw-data'\n",
        "file_key = 'car_dataset.csv'\n",
        "\n",
        "# Generate a signed URL to access the S3 file\n",
        "s3_url = f\"s3://{bucket_name}/{file_key}\"\n",
        "\n",
        "# Read the file into a pandas DataFrame (using boto3 to fetch it)\n",
        "# Using boto3's s3 client to get the file as a CSV\n",
        "obj = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
        "data = obj['Body'].read().decode('utf-8')  # Read the file as a string\n",
        "\n",
        "# Use io.StringIO to read the string data as if it were a file\n",
        "df_pandas = pd.read_csv(io.StringIO(data))  # Convert to pandas DataFrame\n",
        "\n",
        "# Convert pandas DataFrame to PySpark DataFrame\n",
        "df = spark.createDataFrame(df_pandas)\n",
        "# Get the shape (row count and column count)\n",
        "row_count = df.count()  # Number of rows\n",
        "column_count = len(df.columns)  # Number of columns\n",
        "# Log the shape of the DataFrame\n",
        "log_message(message=f\"Shape of the DataFrame: ({row_count}, {column_count})\")\n",
        "# Log and print the top 5 rows of the DataFrame as a table\n",
        "log_show_output(spark_df=df, title=\"Top 5 Rows of the DataFrame\")\n"
      ],
      "metadata": {
        "id": "OqggUVEpySOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ac35a52-3015-464f-876f-e37452b8aae1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:botocore.credentials:Found credentials in environment variables.\n",
            "INFO:root:Shape of the DataFrame: (30781, 26)\n",
            "INFO:root:=== Top 5 Rows of the DataFrame ===\n",
            "INFO:root:+------------+--------------------------------------------------------------------------------------------+------------------+----------------------------------+---------+--------+----------------+---------------------+-------------+-------------+--------+------------+----------------+----------------+-------------------+---------+-----------+----------+---------------+---------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+---------+---------+-----------+--------------------------+\n",
            "|         id | url                                                                                        | region           | region_url                       |   price |   year | manufacturer   | model               | condition   | cylinders   | fuel   |   odometer | title_status   | transmission   | VIN               | drive   | size      | type     | paint_color   | image_url                                                           | description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |   county | state   |     lat |      long | posting_date             |\n",
            "+============+============================================================================================+==================+==================================+=========+========+================+=====================+=============+=============+========+============+================+================+===================+=========+===========+==========+===============+=====================================================================+===============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================+==========+=========+=========+===========+==========================+\n",
            "| 7313747540 | https://baltimore.craigslist.org/cto/d/baltimore-dodge-caravan/7313747540.html             | baltimore        | https://baltimore.craigslist.org |    4800 |   2014 | dodge          | grand caravan sport | like new    | 6 cylinders | gas    |     200000 | clean          | automatic      | NaN               | 4wd     | full-size | mini-van | white         | https://images.craigslist.org/00n0n_bI7zJH3KbPez_0lM0t2_600x450.jpg | Great running vehicle  All maintenance performed on schedule                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |      nan | md      | 39.3446 |  -76.6794 | 2021-04-28T08:05:19-0400 |\n",
            "+------------+--------------------------------------------------------------------------------------------+------------------+----------------------------------+---------+--------+----------------+---------------------+-------------+-------------+--------+------------+----------------+----------------+-------------------+---------+-----------+----------+---------------+---------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+---------+---------+-----------+--------------------------+\n",
            "| 7311507556 | https://boulder.craigslist.org/cto/d/boulder-2011-ford-mustang-manual/7311507556.html      | boulder          | https://boulder.craigslist.org   |   10000 |   2011 | ford           | mustang             | good        | 6 cylinders | gas    |     138725 | clean          | manual         | NaN               | rwd     | NaN       | coupe    | silver        | https://images.craigslist.org/00h0h_dvpP4JFvhj7z_1320MM_600x450.jpg | Great car for sale for someone looking for a fun vehicle. I bought this in high school and when I originally bought it the front bumper was completely destroyed and it had one side scoop taped on. After I purchased it I got a new bumper professionally installed by Maaco which cost me $1300. I also took off the fake side scoop but the way the previous owner put it on left some marks which I couldn't fully remove. There's minor scratches in different places on the exterior and the roof has minor hail damage, again from the previous owner. Everything is mechanically sound and it's on a good set of tires. It has great audio and is pretty quick! I've posted a link so you can see it startup and hear the exhaust. Selling because I'm looking to get a truck to be able to haul things. The best way to reach me is to text me @ 30three 3zero4 three116. Cash only, no exceptions. No low ball offers.  Video Link: https://youtu.be/M6aZ3MnU_yA   Features:  Bassani Exhaust GT-500 Cervinis Hood GT-500 Spoiler Cold-Air Intake Bluetooth Audio Color-changing speedometer and ambient lights                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |      nan | co      | 40.0172 | -105.285  | 2021-04-23T13:12:19-0600 |\n",
            "+------------+--------------------------------------------------------------------------------------------+------------------+----------------------------------+---------+--------+----------------+---------------------+-------------+-------------+--------+------------+----------------+----------------+-------------------+---------+-----------+----------+---------------+---------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+---------+---------+-----------+--------------------------+\n",
            "| 7302115120 | https://madison.craigslist.org/ctd/d/oconomowoc-2013-ford-econoline-cargo/7302115120.html  | madison          | https://madison.craigslist.org   |   11785 |   2013 | ford           | econoline cargo van | good        | 8 cylinders | gas    |     137028 | clean          | automatic      | 1FTSE3EL4DDA34648 | rwd     | mid-size  | van      | custom        | https://images.craigslist.org/00A0A_9cumpZsy1eGz_0cU09G_600x450.jpg | 2013 Ford Econoline Cargo Van E-350 Super Duty Commercial - $11,785  2013 - Ford - Econoline Cargo Van - E-350 Super Duty Commercial with 137,028 miles.  Stock #: 1145 VIN:  1FTSE3EL4DDA34648  It has a RWD   Automatic Transmission with School Bus Exterior.  This    local  WI Cargo Van  has a   5.4L V8  SFI SOHC 16V Engine.  *  Clean, accident-free CarFax history! *  Hard-to-find one-ton van with the 5.4L V8! *  Equipped with cruise control, Bluetooth, cargo cabinets, cargo divider, and much more..*  If you are viewing this listing at a site other than ours..  Please visit my website at otowneautos com for the most up-to-date, most detailed, and highest-quality images!* This vehicle has undergone a complete safety inspection and you receive a copy of it.* Extended Warranties Available.* You Receive a Copy of the Carfax Report on Every Vehicle.* CarFax Clean - Clean-Title Guarantee Policy on Every Vehicle.* We welcome trade in's of any type including motorcycles, snowmobiles, boats, ATVs, or whatever you may have to trade.* Immediate Low APR Financing Available.* We Are an Accredited Member of the Better Business Bureau.* O'Towne Auto Sales provides you with decades of experience and integrity.* Stop in for a test drive or call (262)-567-1880  See at   O'Towne Auto N56 W39369 Wisconsin Ave Oconomowoc, WI 53066 Call us at 262-567-1880 to set up a test Drive or for more information . BatchID: 62J2JDLYEBID: 16882797                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |      nan | wi      | 43.1228 |  -88.5279 | 2021-04-05T11:30:44-0500 |\n",
            "+------------+--------------------------------------------------------------------------------------------+------------------+----------------------------------+---------+--------+----------------+---------------------+-------------+-------------+--------+------------+----------------+----------------+-------------------+---------+-----------+----------+---------------+---------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+---------+---------+-----------+--------------------------+\n",
            "| 7311490812 | https://ocala.craigslist.org/ctd/d/tampa-2012-nissan-juke-sl-awdwe-finance/7311490812.html | ocala            | https://ocala.craigslist.org     |    7990 |   2012 | nissan         | juke                | excellent   | 4 cylinders | gas    |     120056 | clean          | automatic      | JN8AF5MVXCT110566 | 4wd     | mid-size  | SUV      | white         | https://images.craigslist.org/00M0M_lAOAPwJNZeHz_0cU09G_600x450.jpg | Ready To Upgrade Your Ride Today? We Make It Fast & Easy!\t\tCall (or text) ☏ (813) 453−3276\t\t \t\t\t \t\tUnique Motors of Tampa\t\t \t\t8440 N Florida Ave, Tampa, FL 33604\t\tCopy & Paste the URL belowto view more information!http://uniquemotorsoftampa.com\t\t \t \t\t \t  ⭐ Great Bank Financing Options Available ⭐TENEMOS MUCHAS CAMIONETAS ??✅*Credit repair ?✅*Bad credit experts?✅*First time buyers program??✅*Export nation and worldwide??✅*BHPH??✅*Passport only??✅*Job letters as proof of income??✅*Cash deals??✅*Bank drafts??✅*We pay top money for your trade⭐⭐⭐⭐⭐⭐⭐⭐⭐⭐ BUY HERE PAY HERE ⭐⭐⭐⭐⭐⭐⭐⭐⭐⭐Are you already tired of checking out each bad credit car loan available near you?Have they been repeatedly turning you down because of your bad credit? Do you feel hopeless already?✅ Here, we guarantee financing regardless of credit history.✅ We are the bank, on the lot financing.✅ We finance your credibility, not your past.🎈🎈🎈 As you pay your auto loan, we help you rebuild your credit. 🎈🎈🎈100% APPROVAL with OACWe Guarantee It! No matter your financial situation, you will drive off the lot in a new car, today.✅ Bad Credit✅ No Credit ✅ Repossession✅ Bankruptcy✅ Foreclosure✅ SSI✅ Disability✅ Government AssistanceYOU'RE APPROVED!🚗 🚕 🚙  Unique Motors of Tampa   🚗 🚕 🚙☎ CALL OR TEXT (813) 453−3276🔴  BAD CREDIT, GOOD CREDIT WE HAVE A VARIETY OF OPTIONS FOR YOU!!!🔵 IN-HOUSE FINANCING. 🔴 WITH OVER TWO-DOZEN LENDERS AVAILABLE, WE CAN PROVIDE A FINANCING SOLUTION TO MOST ANY CREDIT HISTORY.🔵 WARRANTY AVAILABLE🚘 TRADE/SELL/BUY ✅ GAP INSURANCE AVAILABLE ✅ FIRST TIME BUYER, CREDIT PROGRAM↪ CHECK OUT OUR INVENTORY AThttp://uniquemotorsoftampa.comIF YOU WORK THEN YOU DRIVE! NO SOCIAL SECURITY NUMBER NECESSARY FOR APPROVALS.☑ No Social Security Customers Welcome☑ Low Rates starting at 1.99%☑ Low Monthly Payments☑ Bad or No Credit Accepted☑ Bankruptcy☑ Repos☑ No Social NO PROBLEM☑ Open Car Loans Accepted☑ No Down Payment (W.A.C.)☑ Trade-Ins Accepted \t Optional Equipment : Other   • Dual Air Bags Front Head and Sides   • Active Belts   • All Wheel ABS  Call (or text)  (813) 453−3276 / (813) 453−3276 for quick answers to your questions about this NISSAN JUKE Sl.    ***** NISSAN JUKE Sl SUV *****    2013, 2014, 2015, 2012, 2011, 2010, 2009, NISSAN JUKE, 350Z, Altima, Altima Hybrid, Armada, Frontier, GT-R, Maxima, Murano, Pathfinder, Quest, Rogue, Sentra, Titan, Versa, Xterra, 370Z, Cube, Juke, Leaf, Murano CrossCabriolet, NV Cargo, NV Passenger, NV Cargo NV1500, NV Cargo NV2500 HD, NV Passenger NV3500 HD, NV Cargo NV3500 HD, NV200, Versa Note, Pathfinder Hybrid, Rogue Select   Disclaimer : All advertised prices exclude government fees and taxes, any finance charges, any dealer document preparation charge, and any emission testing charge. The price for listed vehicles as equipped does not include charges such as: License, Title, Registration Fees, State or Local Taxes, Dealer Prep, Smog Fees, Credit Investigation, Optional Credit Insurance, Physical Damage of Liability Insurance, or Delivery Fees. DEALER makes no representations, expressed or implied, to any actual or prospective purchaser or owner of this vehicle as to the existence, ownership, accuracy, description or condition of the listed vehicle's equipment, accessories, price, specials or any warranties. Any and all differences must be addressed prior to the sale of this vehicle. Furthermore, Dealer is not responsible for any errors and omissions inadvertently present in our website regarding vehicle pricing and availability or any other items not covered in this disclosure.    Ready To Upgrade Your Ride Today? We Make It Fast & Easy!Call (or text) ☏ (813) 453−3276  Unique Motors of Tampa 8440 N Florida Ave, Tampa, FL 33604Copy & Paste the URL belowto view more information!http://uniquemotorsoftampa.com    2012 12 *NISSAN* *JUKE* *Cheap Sl* \t\t*Like New 2012 Sl SUV* *1.6L* \t\t*Must See 2012 NISSAN JUKE Sl Gasoline - \t\t2012 NISSAN JUKE  juke JUKE Sl Gasoline SUV Cheap -  \t\t2012 NISSAN JUKE (Sl) Carfax Gasoline 1.6L -  \t\t2012 NISSAN JUKE Sl SUV 1.6L Gasoline  -  \t\tNISSAN JUKE Sl SUV   \t\t*SCHEDULE YOUR TEST DRIVE 2012 NISSAN JUKE  1.6L Sl Gasoline SUV*   \t\t*NISSAN* *JUKE* 2012 NISSAN JUKE Sl Gasoline SUV   \t\t*2012 NISSAN JUKE Sl  \t\t*Unique Motors of Tampa* *Call (or text) us today at (813) 453−3276.* \t\t2013 NISSAN JUKE Sl 1.6L - \t\tHave you seen this 2014 NISSAN JUKE Sl SUV ?  \t\tMust See 2015 NISSAN JUKE  Sl Gasoline SUV  \t\t*For Sale JUKE* *JUKE* *Carfax Sl Gasoline SUV  \t\tCome test drive this amazing *NISSAN* *JUKE* *(SL)* *Gasoline* SUV Sl SUV Gasoline SUV Gasoline* \t\t*(NISSAN)* *(JUKE)* *Sl* *1.6L* *(GASOLINE)* *Bad Credit* \t\t*Gasoline* *SUV*  *Super Vehicle Gasoline Call (or text) this number (813) 453−3276* *1.6L* *Unique Motors of Tampa* * Good Credit* \t\t2012 2011 2010 2009  \t\t*This vehicle is a used NISSAN JUKE* *No Credit* \t\t*It is like New Sl* *1.6L Gasoline*  \t\t*Gasoline* 2008 2007 2006 2005 2004 2003                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |      nan | fl      | 28.027  |  -82.4599 | 2021-04-23T14:45:23-0400 |\n",
            "+------------+--------------------------------------------------------------------------------------------+------------------+----------------------------------+---------+--------+----------------+---------------------+-------------+-------------+--------+------------+----------------+----------------+-------------------+---------+-----------+----------+---------------+---------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+---------+---------+-----------+--------------------------+\n",
            "| 7310297143 | https://hickory.craigslist.org/ctd/d/hickory-2018-kia-forte-lx-sedan-4d/7310297143.html    | hickory / lenoir | https://hickory.craigslist.org   |   15990 |   2018 | kia            | forte lx sedan 4d   | good        | NaN         | gas    |      19466 | clean          | other          | 3KPFK4A7XJE226837 | fwd     | NaN       | sedan    | brown         | https://images.craigslist.org/00N0N_1xMPvfxRAIdz_0gw0co_600x450.jpg | Carvana is the safer way to buy a car During these uncertain times, Carvana is dedicated to ensuring safety for all of our customers. In addition to our 100% online shopping and selling experience that allows all customers to buy and trade their cars without ever leaving the safety of their house, we’re providing touchless delivery that make all aspects of our process even safer. Now, you can get the car you want, and trade in your old one, while avoiding person-to-person contact with our friendly advocates. There are some things that can’t be put off. And if buying a car is one of them, know that we’re doing everything we can to keep you keep moving while continuing to put your health safety, and happiness first. Vehicle Stock# 2000927208📱 Want to instantly check this car’s availability? Call us at  828-483-5442Just text that stock number to 855-976-4304 or head to  and plug it into the search bar!Get PRE-QUALIFIED for your auto loan in 2 minutes - no hit to your credit:http://finance.carvanaauto.com/7047205-74595Looking for more cars like this one? We have 218 Kia Forte in stock for as low as $10990!Why buy with Carvana? We have one standard: the highest. Take a look at just some of the qualifications all of our cars must meet before we list them.150-POINT INSPECTION: We put each vehicle through a 150-point inspection so that you can be 100% confident in its quality and safety. See everything that goes into our inspections at:NO REPORTED ACCIDENTS: We do not sell cars that have been in a reported accident or have a frame or structural damage.7 DAY TEST OWN MONEY BACK GUARANTEE: Every Carvana car comes with a 7-day money-back guarantee. Why? It takes more than 15-minutes to make a decision on your next car. Learn more about test owning at http://about.carvanaauto.comFLEXIBLE FINANCING, TRADE INS WELCOME: We’re all about real-time financing without the middle man. Need financing? Pick a combination of down and monthly payments that work for you. Have a trade-in? We’ll give you a value in 2 minutes. Check out everything about our financing at:http://finance.carvanaauto.com/7047205-74595COST SAVINGS: Carvana's business model has fewer expenses and no bloated fees compared to your local dealership. See how much we can save you at http://about.carvanaauto.comPREMIUM DETAIL: We go the extra mile so that your car is looking as good as new. There are a lot of specifics that we won’t list here (we wash, clean, buff, paint, polish, wax, seal), but trust us that when your car arrives, it’s going to look sweet.Vehicle Info for Stock# 2000927208Trim: LX Sedan 4D sedanMileage: 19k milesExterior Color: BrownInterior Color: BLACKEngine: Nu 2.0L I4 147hp 132ft. lbs.Drive: fwdTransmission: VIN: 3KPFK4A7XJE226837Dealer Disclosure: Price excludes tax, title, and registration (which we handle for you).Disclaimer: You agree that by providing your phone number, Carvana, or Carvana’s authorized representatives*, may call and/or send text messages (including by using equipment to automatically dial telephone numbers) about your interest in a purchase, for marketing/sales purposes, or for any other servicing or informational purpose related to your account. You do not have to consent to receiving calls or texts to purchase from Carvana. While every reasonable effort is made to ensure the accuracy of the information for this Kia Forte, we are not responsible for any errors or omissions contained in this ad. Please verify any information in question with Carvana at 828-483-5442*Including, but not limited to, Bridgecrest Credit Company, GO Financial and SilverRock Automotive.*Kia* *Forte* *LX* *Kia* *Forte* *EX* *Kia* *Forte* *SX* *Kia* *Forte* *LPI* *Hybrid* *Kia* *Forte* *KOUP* *Coupe* *Sedan* 2022  2021  2020  2019  2018  2017  2016  2015  2014  2013  2012  2011  2010  2009  2008  2007  2006  2005  2004  2003  2002  2001  2000   22  21    19  18  17  16  15  14  13  12  11  10  09  08  07  06  05  04  03  02  01  00 |      nan | nc      | 35.73   |  -81.32   | 2021-04-21T10:11:05-0400 |\n",
            "+------------+--------------------------------------------------------------------------------------------+------------------+----------------------------------+---------+--------+----------------+---------------------+-------------+-------------+--------+------------+----------------+----------------+-------------------+---------+-----------+----------+---------------+---------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------+---------+---------+-----------+--------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the specified columns from the DataFrame\n",
        "df = df.drop('url', 'image_url','region', 'region_url', 'VIN', 'ID','description','drive','fuel','title_status')"
      ],
      "metadata": {
        "id": "l6R_esjog3gw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Define the fraction of the data you want to sample\n",
        "# For example, if you want to sample 10% of the data:\n",
        "fraction = 0.4  # Adjust this based on your needs, 0.1 means 10% of the data\n",
        "\n",
        "# Step 2: Perform the random sampling\n",
        "df = df.sample(fraction=fraction, seed=42)  # Set a seed for reproducibility\n",
        "\n",
        "# Step 3: Check the number of rows after sampling\n",
        "row_count = df.count()\n",
        "print(f\"Rows after random sampling: {row_count}\")\n",
        "log_show_output(spark_df=df, title=\"Top 5 Rows of the DataFrame\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBffsvSWvdR1",
        "outputId": "1d64fb72-2b4f-41ea-d94a-a8b9548b199c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:=== Top 5 Rows of the DataFrame ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows after random sampling: 12349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:+---------+--------+----------------+---------------+-------------+-------------+------------+----------------+-----------+--------+---------------+----------+---------+---------+-----------+--------------------------+\n",
            "|   price |   year | manufacturer   | model         | condition   | cylinders   |   odometer | transmission   | size      | type   | paint_color   |   county | state   |     lat |      long | posting_date             |\n",
            "+=========+========+================+===============+=============+=============+============+================+===========+========+===============+==========+=========+=========+===========+==========================+\n",
            "|    7990 |   2012 | nissan         | juke          | excellent   | 4 cylinders |     120056 | automatic      | mid-size  | SUV    | white         |      nan | fl      | 28.027  |  -82.4599 | 2021-04-23T14:45:23-0400 |\n",
            "+---------+--------+----------------+---------------+-------------+-------------+------------+----------------+-----------+--------+---------------+----------+---------+---------+-----------+--------------------------+\n",
            "|   18980 |   2013 | subaru         | impreza wrx   | NaN         | 4 cylinders |     100724 | manual         | NaN       | wagon  | silver        |      nan | wa      | 47.6929 | -117.412  | 2021-05-04T18:00:56-0700 |\n",
            "+---------+--------+----------------+---------------+-------------+-------------+------------+----------------+-----------+--------+---------------+----------+---------+---------+-----------+--------------------------+\n",
            "|   28980 |   2012 | ford           | f-150         | NaN         | 8 cylinders |      93781 | automatic      | NaN       | truck  | black         |      nan | wa      | 47.6929 | -117.412  | 2021-04-15T19:31:18-0700 |\n",
            "+---------+--------+----------------+---------------+-------------+-------------+------------+----------------+-----------+--------+---------------+----------+---------+---------+-----------+--------------------------+\n",
            "|    6900 |   2009 | gmc            | sierra        | fair        | 8 cylinders |     256000 | automatic      | full-size | truck  | red           |      nan | oh      | 39.2353 |  -84.4619 | 2021-04-20T19:07:51-0400 |\n",
            "+---------+--------+----------------+---------------+-------------+-------------+------------+----------------+-----------+--------+---------------+----------+---------+---------+-----------+--------------------------+\n",
            "|   18499 |   2010 | ram            | 1500 big horn | excellent   | 8 cylinders |     128290 | automatic      | full-size | truck  | black         |      nan | ri      | 41.8221 |  -71.3539 | 2021-04-23T11:15:21-0400 |\n",
            "+---------+--------+----------------+---------------+-------------+-------------+------------+----------------+-----------+--------+---------------+----------+---------+---------+-----------+--------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum as _sum, when, isnan\n",
        "\n",
        "# Replace non-standard null values with None\n",
        "df = df.replace([\"\", \"N/A\", \"None\", \"null\",\"NaN\"], None)\n",
        "\n",
        "# Check for null and NaN counts for each column\n",
        "null_counts = df.select([\n",
        "    _sum(when(col(c).isNull() | isnan(col(c)), 1).otherwise(0)).alias(c)\n",
        "    for c in df.columns\n",
        "])\n",
        "null_counts_row = null_counts.collect()[0].asDict()\n",
        "log_show_output(spark_df=df, title=\"Top 5 Rows of the DataFrame\")\n",
        "log_message(message=null_counts_row)"
      ],
      "metadata": {
        "id": "9ZKxtXXvckzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d07ca31-37c7-4a34-f195-02b5a1099827"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:=== Top 5 Rows of the DataFrame ===\n",
            "INFO:root:+---------+--------+----------------+---------------+-------------+-------------+------------+----------------+-----------+--------+---------------+----------+---------+---------+-----------+--------------------------+\n",
            "|   price |   year | manufacturer   | model         | condition   | cylinders   |   odometer | transmission   | size      | type   | paint_color   |   county | state   |     lat |      long | posting_date             |\n",
            "+=========+========+================+===============+=============+=============+============+================+===========+========+===============+==========+=========+=========+===========+==========================+\n",
            "|    7990 |   2012 | nissan         | juke          | excellent   | 4 cylinders |     120056 | automatic      | mid-size  | SUV    | white         |      nan | fl      | 28.027  |  -82.4599 | 2021-04-23T14:45:23-0400 |\n",
            "+---------+--------+----------------+---------------+-------------+-------------+------------+----------------+-----------+--------+---------------+----------+---------+---------+-----------+--------------------------+\n",
            "|   18980 |   2013 | subaru         | impreza wrx   |             | 4 cylinders |     100724 | manual         |           | wagon  | silver        |      nan | wa      | 47.6929 | -117.412  | 2021-05-04T18:00:56-0700 |\n",
            "+---------+--------+----------------+---------------+-------------+-------------+------------+----------------+-----------+--------+---------------+----------+---------+---------+-----------+--------------------------+\n",
            "|   28980 |   2012 | ford           | f-150         |             | 8 cylinders |      93781 | automatic      |           | truck  | black         |      nan | wa      | 47.6929 | -117.412  | 2021-04-15T19:31:18-0700 |\n",
            "+---------+--------+----------------+---------------+-------------+-------------+------------+----------------+-----------+--------+---------------+----------+---------+---------+-----------+--------------------------+\n",
            "|    6900 |   2009 | gmc            | sierra        | fair        | 8 cylinders |     256000 | automatic      | full-size | truck  | red           |      nan | oh      | 39.2353 |  -84.4619 | 2021-04-20T19:07:51-0400 |\n",
            "+---------+--------+----------------+---------------+-------------+-------------+------------+----------------+-----------+--------+---------------+----------+---------+---------+-----------+--------------------------+\n",
            "|   18499 |   2010 | ram            | 1500 big horn | excellent   | 8 cylinders |     128290 | automatic      | full-size | truck  | black         |      nan | ri      | 41.8221 |  -71.3539 | 2021-04-23T11:15:21-0400 |\n",
            "+---------+--------+----------------+---------------+-------------+-------------+------------+----------------+-----------+--------+---------------+----------+---------+---------+-----------+--------------------------+\n",
            "INFO:root:{'price': 0, 'year': 31, 'manufacturer': 546, 'model': 136, 'condition': 5047, 'cylinders': 5101, 'odometer': 131, 'transmission': 70, 'size': 8873, 'type': 2645, 'paint_color': 3684, 'county': 12349, 'state': 0, 'lat': 195, 'long': 195, 'posting_date': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, sum as _sum, when, isnan\n",
        "# Get the total number of rows in the DataFrame\n",
        "total_rows = df.count()\n",
        "# Calculate the percentage of nulls and NaNs in each column\n",
        "null_percentage = (\n",
        "    df.select([\n",
        "        (_sum(when(col(c).isNull() | isnan(col(c)), 1).otherwise(0)) / total_rows).alias(c)\n",
        "        for c in df.columns\n",
        "    ])\n",
        ")\n",
        "# Collect the percentages into a dictionary for evaluation\n",
        "null_percentage_dict = null_percentage.collect()[0].asDict()\n",
        "# Identify columns to drop (null percentage > 40%)\n",
        "columns_to_drop = [col for col, perc in null_percentage_dict.items() if perc > 0.4]\n",
        "# Drop the identified columns\n",
        "df_cleaned = df.drop(*columns_to_drop)\n",
        "# Log the dropped columns and cleaned DataFrame schema\n",
        "log_message(message=f\"Columns dropped: {columns_to_drop}\")\n",
        "log_message(message=df_cleaned.printSchema())  # Log the schema directly without embedding in log_message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZGr7Rbvdwaf",
        "outputId": "0a831274-bfba-4387-d35a-28d633eaf19e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Columns dropped: ['condition', 'cylinders', 'size', 'county']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- price: long (nullable = true)\n",
            " |-- year: double (nullable = true)\n",
            " |-- manufacturer: string (nullable = true)\n",
            " |-- model: string (nullable = true)\n",
            " |-- odometer: double (nullable = true)\n",
            " |-- transmission: string (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- paint_color: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- lat: double (nullable = true)\n",
            " |-- long: double (nullable = true)\n",
            " |-- posting_date: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the shape of the DataFrame after dropping the columns\n",
        "num_rows = df.count()\n",
        "num_columns = len(df.columns)\n",
        "log_message(message=f\"Shape of the dataset after dropping columns: ({num_rows}, {num_columns})\")"
      ],
      "metadata": {
        "id": "mOeuleiwbj1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63ef690e-fca0-409c-9978-b42be20b37cd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Shape of the dataset after dropping columns: (12349, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned = df_cleaned.dropna()\n",
        "# Get the shape of the DataFrame after dropping the columns\n",
        "num_rows = df_cleaned.count()\n",
        "num_columns = len(df_cleaned.columns)\n",
        "log_message(message=f\"Shape of the dataset after dropping columns: ({num_rows}, {num_columns})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwqqqbRgS5BJ",
        "outputId": "d3bcc5c9-c76d-406e-8440-db4fcf67a06e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Shape of the dataset after dropping columns: (7244, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "# Cast `price` and `odometer` to float\n",
        "columns_to_cast_float = [\"price\", \"odometer\"]\n",
        "for column in columns_to_cast_float:\n",
        "    df_cleaned = df_cleaned.withColumn(column, col(column).cast(\"float\"))\n",
        "# Extract numeric part from `cylinders` and cast `year` and `cylinders` to integer\n",
        "df_cleaned = df_cleaned.withColumn(\"year\", col(\"year\").cast(\"int\"))\n",
        "log_message(message=df_cleaned.printSchema())\n",
        "log_show_output(spark_df=df_cleaned, title=\"Top 5 Rows of the DataFrame after type casting\")"
      ],
      "metadata": {
        "id": "uVk9XxuBTYOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11345c07-46c6-4487-bc66-679b5198daf9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:=== Top 5 Rows of the DataFrame after type casting ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- price: float (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- manufacturer: string (nullable = true)\n",
            " |-- model: string (nullable = true)\n",
            " |-- odometer: float (nullable = true)\n",
            " |-- transmission: string (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- paint_color: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- lat: double (nullable = true)\n",
            " |-- long: double (nullable = true)\n",
            " |-- posting_date: string (nullable = true)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:+---------+--------+----------------+---------------+------------+----------------+--------+---------------+---------+---------+-----------+--------------------------+\n",
            "|   price |   year | manufacturer   | model         |   odometer | transmission   | type   | paint_color   | state   |     lat |      long | posting_date             |\n",
            "+=========+========+================+===============+============+================+========+===============+=========+=========+===========+==========================+\n",
            "|    7990 |   2012 | nissan         | juke          |     120056 | automatic      | SUV    | white         | fl      | 28.027  |  -82.4599 | 2021-04-23T14:45:23-0400 |\n",
            "+---------+--------+----------------+---------------+------------+----------------+--------+---------------+---------+---------+-----------+--------------------------+\n",
            "|   18980 |   2013 | subaru         | impreza wrx   |     100724 | manual         | wagon  | silver        | wa      | 47.6929 | -117.412  | 2021-05-04T18:00:56-0700 |\n",
            "+---------+--------+----------------+---------------+------------+----------------+--------+---------------+---------+---------+-----------+--------------------------+\n",
            "|   28980 |   2012 | ford           | f-150         |      93781 | automatic      | truck  | black         | wa      | 47.6929 | -117.412  | 2021-04-15T19:31:18-0700 |\n",
            "+---------+--------+----------------+---------------+------------+----------------+--------+---------------+---------+---------+-----------+--------------------------+\n",
            "|    6900 |   2009 | gmc            | sierra        |     256000 | automatic      | truck  | red           | oh      | 39.2353 |  -84.4619 | 2021-04-20T19:07:51-0400 |\n",
            "+---------+--------+----------------+---------------+------------+----------------+--------+---------------+---------+---------+-----------+--------------------------+\n",
            "|   18499 |   2010 | ram            | 1500 big horn |     128290 | automatic      | truck  | black         | ri      | 41.8221 |  -71.3539 | 2021-04-23T11:15:21-0400 |\n",
            "+---------+--------+----------------+---------------+------------+----------------+--------+---------------+---------+---------+-----------+--------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "# Define the UDF to get ZIP code\n",
        "@udf(StringType())\n",
        "def get_zip_code(lat, lng):\n",
        "    if lat is None or lng is None:  # Check for NoneType values\n",
        "        return None\n",
        "    try:\n",
        "        geolocator = Nominatim(user_agent=\"geoapi\")\n",
        "        location = geolocator.reverse((float(lat), float(lng)), timeout=10)\n",
        "        if location and 'postcode' in location.raw['address']:\n",
        "            return location.raw['address']['postcode']\n",
        "        return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# Filter rows where lat or long is null (optional but recommended)\n",
        "df_cleaned = df_cleaned.filter((col(\"lat\").isNotNull()) & (col(\"long\").isNotNull()))\n",
        "# Apply the UDF to add the zip_code column\n",
        "df_cleaned = df_cleaned.withColumn(\"zip_code\", get_zip_code(col(\"lat\"), col(\"long\")))\n",
        "df_cleaned = df_cleaned.withColumn(\"zip_code\", col(\"zip_code\").cast(\"int\"))\n",
        "# Drop the lat and long columns\n",
        "df_cleaned = df_cleaned.drop(\"lat\", \"long\")\n",
        "# Show the resulting DataFrame\n",
        "log_show_output(spark_df=df_cleaned.select(\"zip_code\"), title=\"Top 5 Rows extracting the zipcode from lat long - FEATURE ENGINEERING\")"
      ],
      "metadata": {
        "id": "QBkGueOcTc5N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77407c66-6d30-4bf9-97e3-8d9ee7e03381"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:=== Top 5 Rows extracting the zipcode from lat long - FEATURE ENGINEERING ===\n",
            "INFO:root:+------------+\n",
            "|   zip_code |\n",
            "+============+\n",
            "|      33604 |\n",
            "+------------+\n",
            "|      99207 |\n",
            "+------------+\n",
            "|      99207 |\n",
            "+------------+\n",
            "|      45215 |\n",
            "+------------+\n",
            "|       2914 |\n",
            "+------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "# Concatenate 'manufacturer' and 'model' into a new column 'car_name'\n",
        "df_cleaned = df_cleaned.withColumn(\n",
        "    \"car_name\", F.concat(F.col(\"manufacturer\"), F.lit(\" \"), F.col(\"model\"))\n",
        ")\n",
        "df_cleaned = df_cleaned.drop('model')\n",
        "# Show the resulting DataFrame with the new 'car_name' column\n",
        "log_show_output(spark_df=df_cleaned.select(\"car_name\"), title=\"Top 5 Rows with car_name column\")"
      ],
      "metadata": {
        "id": "jb9T60DHwDMC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f857dd72-a0aa-4971-e539-10656d859a46"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:=== Top 5 Rows with car_name column ===\n",
            "INFO:root:+--------------------+\n",
            "| car_name           |\n",
            "+====================+\n",
            "| nissan juke        |\n",
            "+--------------------+\n",
            "| subaru impreza wrx |\n",
            "+--------------------+\n",
            "| ford f-150         |\n",
            "+--------------------+\n",
            "| gmc sierra         |\n",
            "+--------------------+\n",
            "| ram 1500 big horn  |\n",
            "+--------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_message(message=df_cleaned.printSchema())"
      ],
      "metadata": {
        "id": "RPMCAJNKThMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b5d2d4-acae-4415-f9d2-3aa43e54637c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- price: float (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- manufacturer: string (nullable = true)\n",
            " |-- odometer: float (nullable = true)\n",
            " |-- transmission: string (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- paint_color: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- posting_date: string (nullable = true)\n",
            " |-- zip_code: integer (nullable = true)\n",
            " |-- car_name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Create a Tokenizer\n",
        "tokenizer = Tokenizer(inputCol=\"car_name\", outputCol=\"car_name_tokens\")\n",
        "\n",
        "# Create a CountVectorizer\n",
        "cv = CountVectorizer(inputCol=\"car_name_tokens\", outputCol=\"car_name_vec\", vocabSize=1000, minDF=2.0)\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline(stages=[tokenizer, cv])\n",
        "\n",
        "# Fit and transform in one step\n",
        "df_vectorized = pipeline.fit(df_cleaned).transform(df_cleaned)\n",
        "\n",
        "# Select only necessary columns\n",
        "df_cleaned = df_vectorized.select(\"*\")\n",
        "df_cleaned = df_cleaned.drop('car_name', 'car_name_tokens')\n",
        "print(df_cleaned.columns)\n",
        "# Show the result\n",
        "log_show_output(spark_df=df_cleaned.select(\"car_name_vec\"), title=\"Top 5 Rows with car_name vector - FEATURE ENGINEERING\")\n"
      ],
      "metadata": {
        "id": "oHCRZJ602L0r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc58eee-07ab-460b-8b92-66692e2ad08a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:=== Top 5 Rows with car_name vector - FEATURE ENGINEERING ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['price', 'year', 'manufacturer', 'odometer', 'transmission', 'type', 'paint_color', 'state', 'posting_date', 'zip_code', 'car_name_vec']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:+----------------------------------------+\n",
            "| car_name_vec                           |\n",
            "+========================================+\n",
            "| (996,[8,338],[1.0,1.0])                |\n",
            "+----------------------------------------+\n",
            "| (996,[20,160,190],[1.0,1.0,1.0])       |\n",
            "+----------------------------------------+\n",
            "| (996,[0,22],[1.0,1.0])                 |\n",
            "+----------------------------------------+\n",
            "| (996,[12,21],[1.0,1.0])                |\n",
            "+----------------------------------------+\n",
            "| (996,[4,13,207,242],[1.0,1.0,1.0,1.0]) |\n",
            "+----------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.columns"
      ],
      "metadata": {
        "id": "uMeNzQtC1JXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d7d40bb-8269-40a8-bde0-33d1ea6d8a8d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['price',\n",
              " 'year',\n",
              " 'manufacturer',\n",
              " 'odometer',\n",
              " 'transmission',\n",
              " 'type',\n",
              " 'paint_color',\n",
              " 'state',\n",
              " 'posting_date',\n",
              " 'zip_code',\n",
              " 'car_name_vec']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# List of columns for StringIndexer\n",
        "columns_to_index = ['transmission', 'paint_color', 'state', 'manufacturer']\n",
        "\n",
        "# Create StringIndexer stages for all columns\n",
        "indexer_stages = [StringIndexer(inputCol=col, outputCol=col + \"_indexed\", handleInvalid=\"keep\") for col in columns_to_index]\n",
        "\n",
        "# Create a pipeline with all StringIndexer stages\n",
        "pipeline = Pipeline(stages=indexer_stages)\n",
        "\n",
        "# Fit the pipeline and transform the DataFrame\n",
        "model = pipeline.fit(df_cleaned)\n",
        "df_cleaned = model.transform(df_cleaned)\n",
        "\n",
        "# Output the columns of the transformed DataFrame\n",
        "log_message(message=df_cleaned.columns)"
      ],
      "metadata": {
        "id": "YcM9IiYoTlrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e96866c-38f7-4e92-eadb-6b37f9da4f2c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:['price', 'year', 'manufacturer', 'odometer', 'transmission', 'type', 'paint_color', 'state', 'posting_date', 'zip_code', 'car_name_vec', 'transmission_indexed', 'paint_color_indexed', 'state_indexed', 'manufacturer_indexed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.ml.linalg import Vector\n",
        "\n",
        "# Define a UDF to sum the vector values\n",
        "def sum_vector(vec):\n",
        "    if vec is not None:\n",
        "        return float(sum(vec.values))  # Sum the values in the sparse vector\n",
        "    return 0.0\n",
        "\n",
        "# Register the UDF\n",
        "sum_vector_udf = udf(sum_vector, DoubleType())\n",
        "\n",
        "# Apply the UDF to create a new column with the sum of the vector values\n",
        "final_df = df_cleaned.withColumn(\"car_name_sum\", sum_vector_udf(\"car_name_vec\"))\n",
        "final_df = final_df.drop('car_name_vec')\n",
        "# Show the resulting DataFrame with the new numeric column\n",
        "log_show_output(spark_df=final_df.select(\"car_name_sum\"), title=\"Top 5 Rows with car_name sum - FEATURE ENGINEERING\")\n"
      ],
      "metadata": {
        "id": "Jf_rgn8BYIy8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd929449-cac2-4669-8ee5-84140f8912cb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:=== Top 5 Rows with car_name sum - FEATURE ENGINEERING ===\n",
            "INFO:root:+----------------+\n",
            "|   car_name_sum |\n",
            "+================+\n",
            "|              2 |\n",
            "+----------------+\n",
            "|              3 |\n",
            "+----------------+\n",
            "|              2 |\n",
            "+----------------+\n",
            "|              2 |\n",
            "+----------------+\n",
            "|              4 |\n",
            "+----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row_count = final_df.count()  # Number of rows\n",
        "column_count = len(final_df.columns)  # Number of columns\n",
        "log_message(message=f\"Shape of the DataFrame after handling nulls and stringtypes: ({row_count}, {column_count})\")"
      ],
      "metadata": {
        "id": "bTlD9_8tTopa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75198c2c-abb6-488f-83b8-76847eee0868"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Shape of the DataFrame after handling nulls and stringtypes: (7244, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "log_message(message=final_df.columns)"
      ],
      "metadata": {
        "id": "rUfeWrL2Tq5L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbba9b8d-45fe-47ec-fff2-3e6293bb3f32"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:['price', 'year', 'manufacturer', 'odometer', 'transmission', 'type', 'paint_color', 'state', 'posting_date', 'zip_code', 'transmission_indexed', 'paint_color_indexed', 'state_indexed', 'manufacturer_indexed', 'car_name_sum']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Open a file to write the results\n",
        "with open('/content/aggregations_results.txt', 'w') as file:\n",
        "\n",
        "    # Select the first 1000 rows from the final DataFrame\n",
        "    df_subset = final_df.limit(1000)\n",
        "\n",
        "    # Select required columns for further analysis\n",
        "    required_columns = [\n",
        "        \"price\", \"manufacturer\", \"odometer\", \"paint_color\", \"state\", \"zip_code\"\n",
        "    ]\n",
        "\n",
        "    # Filter the dataframe to include only the necessary columns and manufacturers of interest\n",
        "    filtered_df = df_subset.select(*required_columns) \\\n",
        "        .filter(F.col(\"manufacturer\").isin(\"audi\", \"honda\", \"bmw\"))\n",
        "\n",
        "    # Aggregation 1: Average Price by Paint Color\n",
        "    file.write(\"Aggregation 1: Average Price by Paint Color\\n\")\n",
        "    paint_color_avg_price_agg = filtered_df.groupBy(\"paint_color\") \\\n",
        "        .agg(F.avg(\"price\").alias(\"avg_price\"))\n",
        "    paint_color_avg_price_agg.show(truncate=False)\n",
        "    paint_color_avg_price_agg.collect()  # Collecting results for writing\n",
        "    file.write(str(paint_color_avg_price_agg.collect()) + '\\n\\n')  # Write to file\n",
        "\n",
        "    # Aggregation 2: Min and Max Price by Manufacturer\n",
        "    file.write(\"Aggregation 2: Min and Max Price by Manufacturer\\n\")\n",
        "    manufacturer_min_max_price_agg = filtered_df.groupBy(\"manufacturer\") \\\n",
        "        .agg(\n",
        "            F.max(\"price\").alias(\"max_price\")\n",
        "        )\n",
        "    manufacturer_min_max_price_agg.show(truncate=False)\n",
        "    manufacturer_min_max_price_agg.collect()  # Collecting results for writing\n",
        "    file.write(str(manufacturer_min_max_price_agg.collect()) + '\\n\\n')  # Write to file\n",
        "\n",
        "    # Aggregation 3: Average Odometer by Manufacturer\n",
        "    file.write(\"Aggregation 3: Average Odometer by Manufacturer\\n\")\n",
        "    odometer_avg_agg = filtered_df.groupBy(\"manufacturer\") \\\n",
        "        .agg(F.avg(\"odometer\").alias(\"avg_odometer\"))\n",
        "    odometer_avg_agg.show(truncate=False)\n",
        "    odometer_avg_agg.collect()  # Collecting results for writing\n",
        "    file.write(str(odometer_avg_agg.collect()) + '\\n\\n')  # Write to file\n",
        "\n",
        "    # Aggregation 4: Count of Cars by State with Zip Code reference\n",
        "    file.write(\"Aggregation 4: Count of Cars by State with Zip Code reference\\n\")\n",
        "    state_car_count_agg = filtered_df.groupBy(\"state\") \\\n",
        "        .agg(\n",
        "            F.count(\"*\").alias(\"car_count\"),\n",
        "            F.first(\"zip_code\").alias(\"zip_code\")  # Fetch a zip code per state\n",
        "        ) \\\n",
        "        .orderBy(F.desc(\"car_count\"))\n",
        "    state_car_count_agg.show()\n",
        "    state_car_count_agg.collect()  # Collecting results for writing\n",
        "    file.write(str(state_car_count_agg.collect()) + '\\n\\n')  # Write to file\n",
        "\n",
        "    # Aggregation 5: Count of Cars by Paint Color\n",
        "    file.write(\"Aggregation 5: Count of Cars by Paint Color\\n\")\n",
        "    paint_color_count_agg = filtered_df.groupBy(\"paint_color\") \\\n",
        "        .agg(F.count(\"*\").alias(\"car_count\"))\n",
        "    paint_color_count_agg.show(truncate=False)\n",
        "    paint_color_count_agg.collect()  # Collecting results for writing\n",
        "    file.write(str(paint_color_count_agg.collect()) + '\\n\\n')  # Write to file\n",
        "\n",
        "print(\"Aggregations have been written to 'aggregations_results.txt'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHWu7nubWFp7",
        "outputId": "37fedf7a-36d7-4a7d-bb7f-d733ff451d28"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------+\n",
            "|paint_color|avg_price         |\n",
            "+-----------+------------------+\n",
            "|grey       |14003.5           |\n",
            "|black      |19922.235294117647|\n",
            "|silver     |16519.619047619046|\n",
            "|white      |19797.58823529412 |\n",
            "|blue       |17254.260869565216|\n",
            "|red        |20205.444444444445|\n",
            "|green      |8627.0            |\n",
            "|brown      |9080.0            |\n",
            "+-----------+------------------+\n",
            "\n",
            "+------------+---------+\n",
            "|manufacturer|max_price|\n",
            "+------------+---------+\n",
            "|audi        |48000.0  |\n",
            "|bmw         |52590.0  |\n",
            "|honda       |39950.0  |\n",
            "+------------+---------+\n",
            "\n",
            "+------------+-----------------+\n",
            "|manufacturer|avg_odometer     |\n",
            "+------------+-----------------+\n",
            "|audi        |67914.34782608696|\n",
            "|bmw         |72961.41025641025|\n",
            "|honda       |113351.25        |\n",
            "+------------+-----------------+\n",
            "\n",
            "+-----+---------+--------+\n",
            "|state|car_count|zip_code|\n",
            "+-----+---------+--------+\n",
            "|   ca|       16|   93117|\n",
            "|   fl|       10|   34104|\n",
            "|   oh|        8|   43229|\n",
            "|   wi|        7|   54703|\n",
            "|   tx|        7|   78767|\n",
            "|   ny|        7|   11550|\n",
            "|   nc|        5|   27511|\n",
            "|   md|        4|   22554|\n",
            "|   nj|        4|    8873|\n",
            "|   pa|        4|   18042|\n",
            "|   tn|        4|   37066|\n",
            "|   ct|        4|    6511|\n",
            "|   mi|        3|   48734|\n",
            "|   wa|        3|   97230|\n",
            "|   va|        3|   24501|\n",
            "|   sc|        3|   29201|\n",
            "|   id|        3|   99207|\n",
            "|   or|        2|   97222|\n",
            "|   ks|        2|   66203|\n",
            "|   de|        2|   19947|\n",
            "+-----+---------+--------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----------+---------+\n",
            "|paint_color|car_count|\n",
            "+-----------+---------+\n",
            "|grey       |10       |\n",
            "|black      |34       |\n",
            "|silver     |21       |\n",
            "|white      |17       |\n",
            "|blue       |23       |\n",
            "|red        |9        |\n",
            "|green      |5        |\n",
            "|brown      |3        |\n",
            "+-----------+---------+\n",
            "\n",
            "Aggregations have been written to 'aggregations_results.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import boto3\n",
        "import botocore\n",
        "import boto3.s3.transfer as s3transfer\n",
        "import pandas as pd\n",
        "import io\n",
        "from tqdm import tqdm\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Function to upload files (as provided earlier)\n",
        "def fast_upload(session, bucketname, s3dir, filelist, progress_func, workers=20):\n",
        "    # Configure botocore to use more connections\n",
        "    botocore_config = botocore.config.Config(max_pool_connections=workers)\n",
        "\n",
        "    # Create the S3 client with the new configuration\n",
        "    s3client = session.client('s3', config=botocore_config)\n",
        "\n",
        "    # Define transfer configuration with threading enabled\n",
        "    transfer_config = s3transfer.TransferConfig(\n",
        "        use_threads=True,\n",
        "        max_concurrency=workers,  # Number of concurrent uploads\n",
        "    )\n",
        "\n",
        "    # Create transfer manager\n",
        "    s3t = s3transfer.create_transfer_manager(s3client, transfer_config)\n",
        "\n",
        "    # Upload files in the list\n",
        "    for src in filelist:\n",
        "        dst = os.path.join(s3dir, os.path.basename(src))\n",
        "\n",
        "        # Start the upload for each file with progress callback\n",
        "        s3t.upload(\n",
        "            src, bucketname, dst,\n",
        "            subscribers=[s3transfer.ProgressCallbackInvoker(progress_func)],\n",
        "        )\n",
        "\n",
        "    # Shutdown the transfer manager after all uploads are completed\n",
        "    s3t.shutdown()  # Wait for all the upload tasks to finish\n",
        "\n",
        "# Function to upload Spark DataFrame as CSV to S3\n",
        "def upload_spark_df_to_s3(spark_df, bucketname, s3dir, file_name, workers=20):\n",
        "    # Convert Spark DataFrame to Pandas DataFrame\n",
        "    pandas_df = spark_df.toPandas()\n",
        "\n",
        "    # Convert the Pandas DataFrame to CSV in memory\n",
        "    csv_buffer = io.StringIO()\n",
        "    pandas_df.to_csv(csv_buffer, index=False)\n",
        "    csv_buffer.seek(0)\n",
        "\n",
        "    # Create a temporary file to upload\n",
        "    tmp_file = \"/tmp/\" + file_name\n",
        "    with open(tmp_file, 'w') as f:\n",
        "        f.write(csv_buffer.getvalue())\n",
        "\n",
        "    # Use fast_upload to upload the CSV file\n",
        "    filelist = [tmp_file]\n",
        "    def progress_func(bytes_transferred, total_bytes):\n",
        "        progress = (bytes_transferred / total_bytes) * 100\n",
        "        print(f\"Progress: {progress:.2f}%\")\n",
        "\n",
        "    # Upload the file using fast_upload\n",
        "    session = boto3.Session()\n",
        "    fast_upload(session, bucketname, s3dir, filelist, progress_func)\n",
        "\n",
        "    # Clean up the temporary file\n",
        "    os.remove(tmp_file)\n",
        "\n",
        "    print(f\"Spark DataFrame successfully uploaded to s3://{bucketname}/{s3dir}/{file_name}\")\n",
        "\n",
        "# Example usage\n",
        "bucketname = \"preprocessed-dataset\"\n",
        "s3dir = \"clean-dataset\"  # S3 directory where the file will be uploaded\n",
        "file_name = \"preprocessed_data.csv\"  # S3 file name\n",
        "\n",
        "# Upload Spark DataFrame to S3 using fast_upload\n",
        "upload_spark_df_to_s3(final_df, bucketname, s3dir, file_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "bIVrWKWHpN4h",
        "outputId": "1ddc4be9-1b78-4cc8-84b0-0c7a9fd44f76"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.4.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/content/spark-3.4.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n",
            "INFO:py4j.clientserver:Closing down clientserver connection\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-d89fb4b2184a>\u001b[0m in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# Upload Spark DataFrame to S3 using fast_upload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0mupload_spark_df_to_s3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucketname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms3dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-d89fb4b2184a>\u001b[0m in \u001b[0;36mupload_spark_df_to_s3\u001b[0;34m(spark_df, bucketname, s3dir, file_name, workers)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupload_spark_df_to_s3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucketname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms3dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Convert Spark DataFrame to Pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mpandas_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Convert the Pandas DataFrame to CSV in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.4.1-bin-hadoop3/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.4.1-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \"\"\"\n\u001b[1;32m   1215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.4.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/content/spark-3.4.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.4.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark Session with S3 configurations\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"S3Upload\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.fast.upload\", \"true\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.multipart.size\", \"100M\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Write DataFrame to S3\n",
        "final_df.coalesce(100).write \\\n",
        "    .option(\"maxRecordsPerFile\", 1000000) \\\n",
        "    .option(\"compression\", \"snappy\") \\\n",
        "    .parquet(f\"s3a://preprocessed-dataset/clean-dataset/preprocessed_data\", mode=\"overwrite\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "-HtkPFEgxVk1",
        "outputId": "0fe9abba-10d0-48ee-8592-b025994a7f0f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o934.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 25 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-d3cb93cc786d>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Write DataFrame to S3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mfinal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"maxRecordsPerFile\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"snappy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.4.1-bin-hadoop3/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1654\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m     def text(\n",
            "\u001b[0;32m/content/spark-3.4.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.4.1-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.4.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o934.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWritingFileFormat(DataSource.scala:454)\n\tat org.apache.spark.sql.execution.datasources.DataSource.planForWriting(DataSource.scala:530)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 25 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.write.csv('/content/output', header=True, mode='overwrite')"
      ],
      "metadata": {
        "id": "iMh70wwwtj77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0dBsRcgizynn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "# import boto3\n",
        "# import os\n",
        "\n",
        "# # Initialize Spark session\n",
        "# spark = SparkSession.builder.appName(\"UploadToS3\").getOrCreate()\n",
        "\n",
        "# # Sample DataFrame (Replace this with your actual DataFrame)\n",
        "# # Replace `final_df` with your actual Spark DataFrame\n",
        "# df = final_df  # Assuming 'final_df' is your Spark DataFrame\n",
        "\n",
        "# # Write the DataFrame to a local CSV file\n",
        "# local_path = \"/tmp/my_spark_dataframe.csv\"\n",
        "# df.write.csv(local_path, header=True, mode=\"overwrite\")\n",
        "\n",
        "# # S3 details\n",
        "# bucket_name = \"preprocessed-dataset\"\n",
        "# region = \"us-east-1\"\n",
        "# file_name = \"my_spark_dataframe.csv\"\n",
        "# s3_path = f\"s3://{bucket_name}/{file_name}\"\n",
        "\n",
        "# # Upload the CSV file to S3 using boto3\n",
        "# s3 = boto3.client('s3', region_name=region)\n",
        "\n",
        "# try:\n",
        "#     # Upload the file to the S3 bucket\n",
        "#     s3.upload_file(local_path, bucket_name, file_name)\n",
        "#     print(f\"File '{file_name}' uploaded successfully to '{s3_path}'.\")\n",
        "\n",
        "#     # Optionally, remove the local file after upload to clean up\n",
        "#     os.remove(local_path)\n",
        "\n",
        "# except boto3.exceptions.S3UploadFailedError as e:\n",
        "#     print(f\"Upload failed: {e}\")\n",
        "# except botocore.exceptions.ClientError as e:\n",
        "#     print(f\"Error uploading file: {e}\")\n"
      ],
      "metadata": {
        "id": "iInadN8t3z6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IhKQ34DZTYb8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}