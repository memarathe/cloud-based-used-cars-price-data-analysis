# -*- coding: utf-8 -*-
"""automation script.sh

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yw4KXKQPeBvw-dbb5g4FFJ4AlUcl3C9e
"""

#!/bin/bash
# Set paths and AWS S3 bucket name
NOTEBOOKS_DIR="/content/big_data_proj/"     # Directory where your notebooks are located
OUTPUT_DIR="/content/output"                # Directory to store output files
S3_BUCKET="s3://preprocessed-dataset/clean-dataset/"        # Your S3 bucket path

# Step 1: Run Jupyter Notebook 1 (Data Cleaning, Feature Engineering, etc.)
echo "Running Notebook 1 - Data Cleaning and Feature Engineering..."
jupyter nbconvert --to notebook --execute --inplace "$NOTEBOOKS_DIR/bda_13_12_2024.ipynb"

# Step 3: Copy the generated processed CSV file to S3 bucket
# Assuming the file is saved as "processed_data.csv" in the output directory
echo "Uploading processed data to S3..."
aws s3 cp "$OUTPUT_DIR/processed_data.csv" "$S3_BUCKET/processed_data.csv"

# Step 5: Run standalone Python script for SQL queries
echo "Running SQL queries via standalone Python script..."
python3 "$NOTEBOOKS_DIR/sparkSQL.py"

# Step 6: Upload processed results to SageMaker (AutoML)
echo "Uploading processed data to SageMaker for AutoML..."
aws s3 cp "$OUTPUT_DIR/processed_data.csv" "$S3_BUCKET/processed_data_for_sagemaker.csv"

# Step 7: Trigger SageMaker AutoML model job (Python script can be used here)
echo "Triggering SageMaker AutoML job..."
python3 "$NOTEBOOKS_DIR/05_trigger_sagemaker_automl.py"

# Step 8: Upload the processed results to QuickSight for analysis
echo "Uploading data to QuickSight..."
# Assuming QuickSight can access your S3 bucket, use the AWS SDK or QuickSight API to create datasets and dashboards
# This step may vary depending on your QuickSight setup.

echo "Pipeline Execution Completed!"